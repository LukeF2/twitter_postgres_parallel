[1mdiff --git a/.github/workflows/tests_denormalized_parallel.yml b/.github/workflows/tests_denormalized_parallel.yml[m
[1mindex 5ce3028..6bf94b6 100644[m
[1m--- a/.github/workflows/tests_denormalized_parallel.yml[m
[1m+++ b/.github/workflows/tests_denormalized_parallel.yml[m
[36m@@ -21,3 +21,4 @@[m [mjobs:[m
         sleep 20[m
         ./load_tweets_parallel.sh[m
         docker compose exec -T pg_denormalized ./run_tests.sh[m
[41m+[m
[1mdiff --git a/README.md b/README.md[m
[1mindex 1b601fb..f35add8 100644[m
[1m--- a/README.md[m
[1m+++ b/README.md[m
[36m@@ -2,9 +2,9 @@[m
 [m
 |     | sequential | parallel |[m
 | --- | ---------- | -------- |[m
[31m-| normalized (unbatched) | ![](https://github.com/LukeF2/twitter_postgres_parallel/workflows/tests_normalized_sequential/badge.svg) | ![](https://github.com/LukeF2/twitter_postgres_parallel/workflows/tests_normalized_parallel/badge.svg) |[m
[31m-| normalized (batched) | ![](https://github.com/LukeF2/twitter_postgres_parallel/workflows/tests_normalizedbatch_sequential/badge.svg) |  ![](https://github.com/LukeF2/twitter_postgres_parallel/workflows/tests_normalizedbatch_parallel/badge.svg) |[m
[31m-| denormalized | ![](https://github.com/LukeF2/twitter_postgres_parallel/workflows/tests_denormalized_sequential/badge.svg) | ![](https://github.com/LukeF2/twitter_postgres_parallel/workflows/tests_denormalized_parallel/badge.svg) |[m
[32m+[m[32m| normalized (unbatched) | ![](https://github.com/afr13dman/twitter_postgres_parallel/workflows/tests_normalized_sequential/badge.svg) | ![](https://github.com/afr13dman/twitter_postgres_parallel/workflows/tests_normalized_parallel/badge.svg) |[m
[32m+[m[32m| normalized (batched) | ![](https://github.com/afr13dman/twitter_postgres_parallel/workflows/tests_normalizedbatch_sequential/badge.svg) |  ![](https://github.com/afr13dman/twitter_postgres_parallel/workflows/tests_normalizedbatch_parallel/badge.svg) |[m
[32m+[m[32m| denormalized | ![](https://github.com/afr13dman/twitter_postgres_parallel/workflows/tests_denormalized_sequential/badge.svg) | ![](https://github.com/afr13dman/twitter_postgres_parallel/workflows/tests_denormalized_parallel/badge.svg) |[m
 [m
 In this assignment, you will learn how to load data into postgres much faster using two techniques:[m
 1. batch loading (i.e. running the INSERT command on more than one row at a time)[m
[36m@@ -280,9 +280,9 @@[m [mEnsure that your runtimes on the lambda server are recorded below.[m
 [m
 |                        | elapsed time (sequential) | elapsed time (parallel)   |[m
 | -----------------------| ------------------------- | ------------------------- |[m
[31m-| `pg_normalized`        |                           |                           | [m
[31m-| `pg_normalized_batch`  |                           |                           | [m
[31m-| `pg_denormalized`      |                           |                           | [m
[32m+[m[32m| `pg_normalized`        | 8m34.645s                 | 1m4.053s                  |[m[41m [m
[32m+[m[32m| `pg_normalized_batch`  | 3m8.212s                  | 0m16.969s                 |[m[41m [m
[32m+[m[32m| `pg_denormalized`      | 0m21.189s                 | 0m2.879s                  |[m[41m [m
 [m
 Then upload a link to your forked github repo on sakai.[m
 [m
[1mdiff --git a/docker-compose.yml b/docker-compose.yml[m
[1mindex b19a575..196b848 100644[m
[1m--- a/docker-compose.yml[m
[1m+++ b/docker-compose.yml[m
[36m@@ -11,7 +11,7 @@[m [mservices:[m
       - POSTGRES_PASSWORD=pass[m
       - PGUSER=postgres[m
     ports:[m
[31m-      - 1323:5432[m
[32m+[m[32m      - 1104:5432[m
 [m
   pg_normalized:[m
     build: services/pg_normalized[m
[36m@@ -23,7 +23,7 @@[m [mservices:[m
       - POSTGRES_PASSWORD=pass[m
       - PGUSER=postgres[m
     ports:[m
[31m-      - 1324:5432[m
[32m+[m[32m      - 1105:5432[m
 [m
   pg_normalized_batch:[m
     build: services/pg_normalized_batch[m
[36m@@ -35,7 +35,7 @@[m [mservices:[m
       - POSTGRES_PASSWORD=pass[m
       - PGUSER=postgres[m
     ports:[m
[31m-      - 1325:5432[m
[32m+[m[32m      - 1106:5432[m
 [m
 volumes:[m
     pg_normalized:[m
[1mdiff --git a/load_denormalized.sh b/load_denormalized.sh[m
[1mindex dc24f9e..e41082a 100755[m
[1m--- a/load_denormalized.sh[m
[1m+++ b/load_denormalized.sh[m
[36m@@ -1,3 +1,3 @@[m
 #!/bin/sh[m
 [m
[31m-unzip -p "$1" | sed 's/\\u0000//g' | psql postgresql://postgres:pass@localhost:1323/ -c "COPY tweets_jsonb (data) FROM STDIN csv quote e'\x01' delimiter e'\x02';"[m
[32m+[m[32munzip -p "$1" | sed 's/\\u0000//g' | psql postgresql://postgres:pass@localhost:1104/ -c "COPY tweets_jsonb (data) FROM STDIN csv quote e'\x01' delimiter e'\x02';"[m
[1mdiff --git a/load_tweets_parallel.sh b/load_tweets_parallel.sh[m
[1mindex 9229d1a..9e0f372 100755[m
[1m--- a/load_tweets_parallel.sh[m
[1m+++ b/load_tweets_parallel.sh[m
[36m@@ -10,9 +10,9 @@[m [mtime echo "$files" | parallel ./load_denormalized.sh[m
 echo '================================================================================'[m
 echo 'load pg_normalized'[m
 echo '================================================================================'[m
[31m-time echo "$files" | parallel python3 load_tweets.py --db="postgresql://postgres:pass@localhost:1324" --inputs={}[m
[32m+[m[32mtime echo "$files" | parallel python3 load_tweets.py --db="postgresql://postgres:pass@localhost:1105" --inputs={}[m
 [m
 echo '================================================================================'[m
 echo 'load pg_normalized_batch'[m
 echo '================================================================================'[m
[31m-time echo "$files" | parallel python3 -u load_tweets_batch.py --db=postgresql://postgres:pass@localhost:1325/ --inputs={}[m
[32m+[m[32mtime echo "$files" | parallel python3 -u load_tweets_batch.py --db=postgresql://postgres:pass@localhost:1106/ --inputs={}[m
[1mdiff --git a/load_tweets_sequential.sh b/load_tweets_sequential.sh[m
[1mindex 019c95b..34b014e 100755[m
[1m--- a/load_tweets_sequential.sh[m
[1m+++ b/load_tweets_sequential.sh[m
[36m@@ -6,19 +6,19 @@[m [mecho '==========================================================================[m
 echo 'load denormalized'[m
 echo '================================================================================'[m
 time for file in $files; do[m
[31m-        unzip -p "$file" | sed 's/\\u0000//g' | psql postgresql://postgres:pass@localhost:1323 -c "COPY tweets_jsonb (data) FROM STDIN csv quote e'\x01' delimiter e'\x02';"[m
[32m+[m[32m    unzip -p "$file" | sed 's/\\u0000//g' | psql postgresql://postgres:pass@localhost:1104 -c "COPY tweets_jsonb (data) FROM STDIN csv quote e'\x01' delimiter e'\x02';"[m
 done[m
 [m
 echo '================================================================================'[m
 echo 'load pg_normalized'[m
 echo '================================================================================'[m
 time for file in $files; do[m
[31m-        python3 load_tweets.py --db="postgresql://postgres:pass@localhost:1324" --inputs="$file"[m
[32m+[m[32m    python3 load_tweets.py --db="postgresql://postgres:pass@localhost:1105" --inputs="$file"[m
 done[m
 [m
 echo '================================================================================'[m
 echo 'load pg_normalized_batch'[m
 echo '================================================================================'[m
 time for file in $files; do[m
[31m-    python3 -u load_tweets_batch.py --db=postgresql://postgres:pass@localhost:1325/ --inputs $file[m
[32m+[m[32m    python3 -u load_tweets_batch.py --db=postgresql://postgres:pass@localhost:1106/ --inputs $file[m
 done[m
[1mdiff --git a/services/pg_normalized_batch/schema.sql b/services/pg_normalized_batch/schema.sql[m
[1mindex e8d9859..4e6397f 100644[m
[1m--- a/services/pg_normalized_batch/schema.sql[m
[1m+++ b/services/pg_normalized_batch/schema.sql[m
[36m@@ -4,7 +4,6 @@[m [mCREATE EXTENSION postgis;[m
 [m
 BEGIN;[m
 [m
[31m-[m
 /*[m
  * Users may be partially hydrated with only a name/screen_name [m
  * if they are first encountered during a quote/reply/mention [m
[36m@@ -14,7 +13,7 @@[m [mCREATE TABLE users ([m
     id_users BIGINT NOT NULL,[m
     created_at TIMESTAMPTZ,[m
     updated_at TIMESTAMPTZ,[m
[31m-    url TEXT,[m
[32m+[m[32m    url TEXT,[m[41m [m
     friends_count INTEGER,[m
     listed_count INTEGER,[m
     favourites_count INTEGER,[m
